# Random Forest
### 🌲 `RandomForestRegressor` 常用超参数详解：

#### 1. **`n_estimators`**（默认：100）
- **含义**：随机森林中树的数量。
- **影响**：
  - 增大可以提升模型稳定性和准确性，但会增加训练时间。
  - 数量太少可能欠拟合，太多可能增加训练成本但提升有限。
- **建议**：开始可设为 100~300，后续根据性能调优。

---

#### 2. **`max_depth`**（默认：None）
- **含义**：树的最大深度。
- **影响**：
  - 控制单棵树的复杂度，限制过拟合。
  - 太小 → 欠拟合；太大 → 过拟合。
- **建议**：通常设置为 10~30，或使用 `None` 让树完全生长（但风险过拟合）。

---

#### 3. **`min_samples_split`**（默认：2）
- **含义**：内部节点再划分所需的最小样本数。
- **影响**：
  - 增大该值可以减少过拟合（强制更大的分组再分裂）。
  - 太小 → 过拟合；太大 → 欠拟合。
- **建议**：可尝试 2、5、10。

---

#### 4. **`min_samples_leaf`**（默认：1）
- **含义**：叶子节点最少样本数。
- **影响**：
  - 增大值会让每个叶子节点包含更多样本，减少过拟合。
- **建议**：可尝试 1、5、10。

---

#### 5. **`max_features`**（默认："auto"/"sqrt"）
- **含义**：在每个分裂点考虑的最大特征数。
- **影响**：
  - 控制每棵树“看见”特征的多少。
  - 减小该值 → 降低相关性，提升泛化能力，但也可能欠拟合。
- **建议**：
  - 对于回归：常用 `"auto"` 或 `"sqrt"`（即 `sqrt(n_features)`）；
  - 可手动设置为整数（如 5）或比例（如 `0.5` 表示用一半特征）。

---

#### 6. **`bootstrap`**（默认：True）
- **含义**：是否采用有放回采样构建子样本（Bagging）。
- **影响**：
  - `True`（默认）更能提升泛化能力；
  - `False` 更像是传统的装袋决策树。

---

#### 7. **`random_state`**（默认：None）
- **含义**：随机数种子，用于结果可复现。
- **建议**：实验调试时设置固定值（如 42），正式使用时可忽略。

---

#### 8. **`n_jobs`**（默认：None）
- **含义**：指定并行处理的CPU核心数。
- **建议**：设为 `-1` 表示使用所有核心，加快训练。

# SVR
### 📌 常用 SVR 超参数（尤其是 `kernel='rbf'` 时）：

| 参数名        | 默认值 | 含义 | 主要影响 | 建议调参范围 |
|---------------|--------|------|----------|--------------|
| **`C`**        | 1.0    | 惩罚系数     | 控制模型对误差的容忍度和平衡复杂度。高 C → 更拟合训练集。 | `0.1` ~ `1000`（对数尺度） |
| **`epsilon`** | 0.1    | ε-不敏感区域 | 在预测与真实值之间的误差小于 ε 的，不算作损失。 | `0.001` ~ `1.0` |
| **`gamma`**   | `'scale'` | RBF核参数    | 控制单个样本对模型的影响范围。小 gamma → 更平滑；大 gamma → 更拟合。 | `0.0001` ~ `10`（对数尺度） |

---

#### 🔍 1. `C`（惩罚系数） — 控制拟合 vs 泛化

- **小 `C`**：
  - 更容忍预测值与真实值之间的偏差。
  - **偏向简单模型** → 可能欠拟合。
- **大 `C`**：
  - 更关注每个点 → 更容易**过拟合**。

###### 类比：
C 就像是老师的严格程度：  
- 宽容老师（小C）对错误不太在意；  
- 严格老师（大C）要求几乎每个样本都尽量对。

---

#### 🔍 2. `epsilon`（ε-不敏感区域）— 控制预测容忍带宽

- **大 `epsilon`**：
  - 模型对误差“不敏感”，训练出来的模型可能更平滑；
  - 但可能**欠拟合**（忽视一些小误差）。
- **小 `epsilon`**：
  - 对误差更敏感，拟合会更精细；
  - 但风险是**过拟合**。

---

#### 🔍 3. `gamma`（核函数参数）— 控制 RBF 核的非线性能力

- **大 `gamma`**：
  - 样本对模型影响范围变小 → 模型更复杂，更容易过拟合。
- **小 `gamma`**：
  - 每个样本影响范围更大 → 模型更平滑，可能欠拟合。

###### 默认值：
- `'scale'` = `1 / (n_features * X.var())`（通常更稳健）；
- `'auto'` = `1 / n_features`。

---


### 总结一句话：

| 想更拟合（精细）→ | 提高 C，减小 epsilon，适中 gamma（别太大） |
|------------------|------------------------------------------------|
| 想更泛化（防过拟合）→ | 减小 C，增大 epsilon，减小 gamma |

---

### 🔧 `SVR` 支持的 Kernel 类型一览：

| Kernel 类型 | 描述 | 常用参数 | 适用场景 |
|-------------|------|----------|----------|
| `'linear'`  | 线性核函数 | `C`, `epsilon` | 特征之间是线性关系；特征很多时也适用（快） |
| `'poly'`    | 多项式核函数 | `degree`, `coef0`, `gamma`, `C`, `epsilon` | 有多项式特征关系，拟合曲线关系 |
| `'rbf'`     | 高斯径向基核函数（默认） | `gamma`, `C`, `epsilon` | 适合大多数非线性关系；泛用性强 |
| `'sigmoid'` | Sigmoid 核函数 | `coef0`, `gamma`, `C`, `epsilon` | 类似神经网络激活函数，应用少 |
| callable    | 自定义核函数 | 你自己写一个 `kernel(x, y)` 函数 | 特定需求，例如图核、字符串核 |

---

### 各种 Kernel 的参数细节：

---

#### 🔹 1. `'linear'` 线性核

```python
SVR(kernel='linear', C=1.0, epsilon=0.1)
```

- 无额外核参数。
- 本质就是线性回归的支持向量形式。
- 快速、可解释性强。

---

#### 🔹 2. `'poly'` 多项式核

```python
SVR(kernel='poly', degree=3, coef0=1, gamma='scale')
```

- `degree`: 多项式的次数（常用：2~5）
- `coef0`: 常数项（影响高阶项影响力）
- `gamma`: 控制单个样本的影响范围
- 通常用于有**幂次关系**或**交互作用项**的场景

> 核函数形式：  
> \( K(x, x') = (\gamma \cdot x^T x' + \text{coef0})^{\text{degree}} \)

---

#### 🔹 3. `'rbf'` 径向基核（高斯核）

```python
SVR(kernel='rbf', C=100, epsilon=0.01, gamma=0.1)
```

- `gamma` 是最重要的参数（影响样本影响范围）
- 适合捕捉大多数复杂的非线性关系
- 泛化能力强，但易过拟合，需调参

---

#### 🔹 4. `'sigmoid'` S型核（神经网络风格）

```python
SVR(kernel='sigmoid', coef0=0, gamma=0.01)
```

- 形式类似于神经网络的激活函数：  
  \( K(x, x') = \tanh(\gamma \cdot x^T x' + \text{coef0}) \)
- 应用较少，一般不如 RBF 表现稳定

---

### 🎯 如何选择 kernel？

| 你的数据特性 | 推荐 kernel |
|--------------|--------------|
| 关系接近线性 | `'linear'` |
| 存在幂次特征或交互作用 | `'poly'` |
| 一般情况或关系复杂未知 | `'rbf'` |
| 想尝试类似神经网络效果 | `'sigmoid'` |


---

### 📊 小结对比：

| kernel | 样条拟合能力 | 参数复杂性 | 通用性 | 风险 |
|--------|----------------|--------------|---------|------|
| linear | ⭐             | ⭐           | ⭐       | 欠拟合 |
| poly   | ⭐⭐⭐           | ⭐⭐⭐         | ⭐⭐      | 过拟合 |
| rbf    | ⭐⭐⭐⭐          | ⭐⭐          | ⭐⭐⭐⭐   | 过拟合 |
| sigmoid| ⭐⭐            | ⭐⭐          | ⭐       | 不稳定 |

---

# XGBoost
### 🎯 `XGBRegressor` 常用超参数

### 🌟 XGBRegressor 核心可调参数一览：

| 类别 | 参数 | 含义 | 影响 |
|------|------|------|------|
| 📏 模型复杂度控制 | `n_estimators`, `max_depth`, `min_child_weight` | 控制树数量与结构复杂度 | 防止过拟合 |
| 📐 学习速度控制 | `learning_rate` | 每次迭代的权重调整步长 | 控制模型收敛和稳定性 |
| 🧱 结构正则 | `gamma`, `subsample`, `colsample_bytree` | 控制节点分裂、采样等 | 抗过拟合、提升泛化能力 |
| 🧠 拟合方式控制 | `booster`, `objective`, `tree_method` | 控制底层实现方式 | 性能优化 |
| 💡 提升精度 | `reg_alpha`, `reg_lambda` | L1/L2 正则项 | 防止模型太复杂 |

---

### 🔍 常调参数详解（最关键的那几个）：

---

#### ✅ 1. `n_estimators` （默认：100）

- 树的数量。
- 越多模型越复杂 → 拟合力强 → 更容易过拟合。
- 与 `learning_rate` 联合调节。

🧪 建议：100~1000（如果用 early_stopping，可以放心多设一点）

---

#### ✅ 2. `learning_rate` / `eta`（默认：0.3）

- 控制每棵树的“贡献权重”，学习步长。
- 越小越稳（但收敛慢）；越大越快（但风险不稳定）。

🧪 建议：
- 最常用：0.01、0.05、0.1
- 小学习率时应 **加大 `n_estimators`**

---

#### ✅ 3. `max_depth`（默认：6）

- 每棵树的最大深度。
- 越大 → 更复杂模型，更易过拟合。
- 对非线性特征很敏感。

🧪 建议：3 ~ 10（一般 6 是个不错起点）

---

#### ✅ 4. `min_child_weight`（默认：1）

- 控制叶子节点最小样本权重总和。
- 增大这个值可以防止模型学到太小的样本块（抗过拟合）。

🧪 建议：
- 初始尝试：1, 3, 5, 10
- 类似 `min_samples_leaf` 的效果

---

#### ✅ 5. `gamma`（默认：0）

- 一个节点在分裂前，要求的最小 loss 降低值。
- 越大 → 节点更难被分裂 → 抑制过拟合。

🧪 建议：0, 0.1, 0.5, 1

---

#### ✅ 6. `subsample`（默认：1）

- 每棵树构建时，用多少比例的训练样本（行采样）。
- 降低它可以提升泛化能力（Bagging 思路）。

🧪 建议：0.5 ~ 1（常用：0.8）

---

#### ✅ 7. `colsample_bytree`（默认：1）

- 构建每棵树时，用多少比例的特征（列采样）。
- 类似随机森林思想，有助于防止过拟合。

🧪 建议：0.5 ~ 1（常用：0.7, 0.8）

---

#### ✅ 8. `reg_alpha`, `reg_lambda`

- L1（lasso）和 L2（ridge）正则项。
- 抑制特征的系数波动，防止模型过于复杂。

🧪 建议：
```python
reg_alpha: 0, 0.1, 1
reg_lambda: 1, 5, 10
```

---

### 📘 附加参数（进阶使用）：

| 参数 | 含义 | 常用设定 |
|------|------|-----------|
| `booster` | 基学习器类型（常用：`gbtree`） | gbtree, gblinear, dart |
| `objective` | 回归任务目标函数 | `reg:squarederror`, `reg:pseudohubererror` |
| `tree_method` | 树构建方法 | `auto`, `exact`, `hist`, `gpu_hist` |